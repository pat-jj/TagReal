{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'head': 'm.01063t',\n",
       " 'tail': 'm.01063t',\n",
       " 'relation': '/location/hud_county_place/place'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Set\n",
    "\n",
    "# read KG txt files\n",
    "\n",
    "f = open('fb60k_graph_raw.txt')\n",
    "data_kg_raw = f.readlines()\n",
    "print(len(data_kg_raw))\n",
    "data_kg = []\n",
    "for line in data_kg_raw:\n",
    "    items = line[:-1].split('\\t')\n",
    "    head, relation, tail = items[0], items[1], items[2]\n",
    "    kg_triple = {'head': head, 'tail': tail, 'relation': relation}\n",
    "    data_kg.append(kg_triple)\n",
    "\n",
    "data_kg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read corpus json files\n",
    "\n",
    "import json\n",
    "\n",
    "f = open('nyt10_train_raw.json')\n",
    "data_corpus = json.load(f)\n",
    "\n",
    "f = open('nyt10_test_raw.json')\n",
    "tmp = json.load(f)\n",
    "\n",
    "data_corpus = data_corpus + tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'sen. charles e. schumer called on federal safety officials yesterday to reopen their investigation into the fatal crash of a passenger jet in belle_harbor , queens , because equipment failure , not pilot error , might have been the cause . ###END###\\n',\n",
       " 'head': {'word': 'queens', 'id': 'm.0ccvx'},\n",
       " 'tail': {'word': 'belle_harbor', 'id': 'm.05gf08'},\n",
       " 'relation': '/location/location/contains'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d4a5b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'word': 'queens', 'id': 'm.0ccvx'},\n",
       " '/location/location/contains',\n",
       " {'word': 'belle_harbor', 'id': 'm.05gf08'},\n",
       " 742536)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_corpus[0]['head'], data_corpus[0]['relation'], data_corpus[0]['tail'], len(data_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b4eff89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/business/company/founders',\n",
       " '/business/company/place_founded',\n",
       " '/business/person/company',\n",
       " '/location/administrative_division/country',\n",
       " '/location/country/administrative_divisions',\n",
       " '/location/location/contains',\n",
       " '/location/neighborhood/neighborhood_of',\n",
       " '/location/us_county/county_seat',\n",
       " '/people/deceased_person/place_of_death',\n",
       " '/people/ethnicity/people',\n",
       " '/people/person/children',\n",
       " '/people/person/ethnicity',\n",
       " '/people/person/nationality',\n",
       " '/people/person/place_lived',\n",
       " '/people/person/religion'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_relation_set = set()\n",
    "\n",
    "rel_list = ['/people/person/nationality', '/location/location/contains', '/people/person/place_lived', \n",
    "            '/people/deceased_person/place_of_death', '/people/person/ethnicity', '/people/ethnicity/people',\n",
    "            '/business/person/company', '/people/person/religion', '/location/neighborhood/neighborhood_of',\n",
    "            '/business/company/founders', '/people/person/children', '/location/administrative_division/country',\n",
    "            '/location/country/administrative_divisions', '/business/company/place_founded', '/location/us_county/county_seat']\n",
    "\n",
    "for i in range(len(rel_list)):\n",
    "    valid_relation_set.add(rel_list[i])\n",
    "    \n",
    "valid_relation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d10102ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 742536/742536 [00:00<00:00, 3224359.48it/s]\n",
      "100%|██████████| 335350/335350 [00:00<00:00, 3231554.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "data_corpus_new = []\n",
    "data_kg_new = []\n",
    "# filter relation\n",
    "for item in tqdm(data_corpus):\n",
    "    # if item['relation'] in valid_relation_set:\n",
    "        data_corpus_new.append(item)\n",
    "\n",
    "for item in tqdm(data_kg):\n",
    "    # if item['relation'] in valid_relation_set:\n",
    "        data_kg_new.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54dd8574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143493, 74035)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_corpus_new), len(data_kg_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3f0e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract entity to label from corpus\n",
    "entity2label = []\n",
    "for item in data_corpus_new:\n",
    "    head = item['head']['id'] + '\\t' + item['head']['word'] + '\\n'\n",
    "    tail = item['tail']['id'] + '\\t' + item['tail']['word'] + '\\n'\n",
    "    entity2label.append(head)\n",
    "    entity2label.append(tail)\n",
    "\n",
    "entity2label = [*set(entity2label)]\n",
    "\n",
    "entity2label_str = ''\n",
    "\n",
    "for item in entity2label:\n",
    "    entity2label_str = entity2label_str + item\n",
    "\n",
    "e2l_file = open(\"../../../../dataset/FB60K-NYT10/entity2label.txt\", 'w', encoding='utf-8')\n",
    "print(entity2label_str, file=e2l_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "origin_train = open(\"../../../../fb_nyt_origin_data/text/train.txt\").readlines()\n",
    "origin_test = open(\"../../../../fb_nyt_origin_data/text/test.txt\").readlines()\n",
    "\n",
    "entity2label = []\n",
    "\n",
    "def extract_e2l(e2l, dataset):\n",
    "    for data in dataset:\n",
    "        its = data.split('\\t')\n",
    "        head_id = its[0]\n",
    "        tail_id = its[1]\n",
    "        head_name = its[2]\n",
    "        tail_name = its[3]\n",
    "        head_ = head_id + '\\t' + head_name + '\\n'\n",
    "        tail_ = tail_id + '\\t' + tail_name + '\\n'\n",
    "        e2l.append(head_)\n",
    "        e2l.append(tail_)\n",
    "\n",
    "extract_e2l(entity2label, origin_train)\n",
    "extract_e2l(entity2label, origin_test)\n",
    "entity2label = [*set(entity2label)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of e2l entities:  69513\n",
      "size of corpus entities:  15989\n",
      "size of kg entities:  34813\n",
      "KG entities not included in e2l:  0\n",
      "corpus entities not included in e2l:  0\n"
     ]
    }
   ],
   "source": [
    "def check_e2l_mapping(e2l, corpus, kg):\n",
    "    kg_entities = []\n",
    "    e2l_entities = []\n",
    "    corpus_entities = []\n",
    "    for it in e2l:\n",
    "        e2l_entities.append(it.split('\\t')[0])\n",
    "    e2l_entities = [*set(e2l_entities)]\n",
    "    print('size of e2l entities: ', len(e2l_entities))\n",
    "\n",
    "    for it in corpus:\n",
    "        corpus_entities.append(it['head']['id'])\n",
    "        corpus_entities.append(it['tail']['id'])\n",
    "\n",
    "    corpus_entities = [*set(corpus_entities)]\n",
    "    print('size of corpus entities: ', len(corpus_entities))\n",
    "\n",
    "    for it in kg:\n",
    "        kg_entities.append(it['head'])\n",
    "        kg_entities.append(it['tail'])\n",
    "\n",
    "    kg_entities = [*set(kg_entities)]\n",
    "    print('size of kg entities: ', len(kg_entities))\n",
    "\n",
    "    outer = set()\n",
    "    for it in kg_entities:\n",
    "        if it not in e2l_entities:\n",
    "            # print(it, 'mapping is not complete')\n",
    "            outer.add(it)\n",
    "\n",
    "    print('KG entities not included in e2l: ', len(outer))\n",
    "\n",
    "    outer = set()\n",
    "    for it in corpus_entities:\n",
    "        if it not in e2l_entities:\n",
    "            # print(it, 'mapping is not complete')\n",
    "            outer.add(it)\n",
    "\n",
    "    print('corpus entities not included in e2l: ', len(outer))\n",
    "\n",
    "    entities = set(kg_entities).union(set(corpus_entities))\n",
    "\n",
    "    return entities\n",
    "\n",
    "kg_corpus_entities = check_e2l_mapping(entity2label, data_corpus_new, data_kg_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entity2label_str = ''\n",
    "\n",
    "for item in entity2label:\n",
    "    if item.split('\\t')[0] in kg_corpus_entities:\n",
    "        entity2label_str = entity2label_str + item\n",
    "\n",
    "e2l_file = open(\"../../../../dataset/FB60K-NYT10/entity2label.txt\", 'w', encoding='utf-8')\n",
    "print(entity2label_str, file=e2l_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3c138cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 742536/742536 [00:01<00:00, 477112.68it/s]\n",
      "100%|██████████| 335350/335350 [00:00<00:00, 545487.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(5583)\n",
    "# make train/test/valid data\n",
    "random.shuffle(data_corpus_new)\n",
    "random.shuffle(data_kg_new)\n",
    "\n",
    "lines = entity2label_str.split('\\n')\n",
    "\n",
    "e2l_mapping = {}\n",
    "for line in lines[:-1]:\n",
    "    e = line.split('\\t')[0]\n",
    "    l = line.split('\\t')[1]\n",
    "    e2l_mapping[e] = l\n",
    "\n",
    "data_train = \"\"\n",
    "data_test = \"\"\n",
    "data_valid = \"\"\n",
    "all_triples_by_name = \"\"\n",
    "\n",
    "count = 0\n",
    "\n",
    "triple_set = set()\n",
    "\n",
    "for item in tqdm(data_corpus_new):\n",
    "    triple = item['head']['id'] + '\\t' + item['relation'] + '\\t' + item['tail']['id'] + '\\n'\n",
    "    # triple_by_name = item['head']['word'] + '\\t' + item['relation'] + '\\t' + item['tail']['word'] + '\\n'\n",
    "    if triple not in triple_set: \n",
    "        triple_set.add(triple)\n",
    "        count += 1\n",
    "        # all_triples_by_name += triple_by_name\n",
    "        if count == 9:\n",
    "            if item['relation'] in valid_relation_set:\n",
    "                data_test += triple\n",
    "            continue\n",
    "        if count == 10:\n",
    "            if item['relation'] in valid_relation_set:\n",
    "                data_valid += triple\n",
    "            count = 0\n",
    "            continue\n",
    "        if item['relation'] in valid_relation_set:\n",
    "            data_train += triple\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "for item in tqdm(data_kg_new):\n",
    "    triple = item['head'] + '\\t' + item['relation'] + '\\t' + item['tail'] + '\\n'\n",
    "    # triple_by_name = e2l_mapping[item['head']] + '\\t' + item['relation'] + '\\t' + e2l_mapping[item['tail']] + '\\n'\n",
    "    if triple not in triple_set:\n",
    "        triple_set.add(triple)\n",
    "        count += 1\n",
    "        # all_triples_by_name += triple_by_name\n",
    "        if count == 9:\n",
    "            if item['relation'] in valid_relation_set:\n",
    "                data_test += triple\n",
    "            continue\n",
    "        if count == 10:\n",
    "            if item['relation'] in valid_relation_set:\n",
    "                data_valid += triple\n",
    "            count = 0\n",
    "            continue\n",
    "        if item['relation'] in valid_relation_set:\n",
    "            data_train += triple\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "train = open(\"../train.txt\", 'w', encoding='utf-8')\n",
    "print(data_train, file=train)\n",
    "\n",
    "test = open(\"../test.txt\", 'w', encoding='utf-8')\n",
    "print(data_test, file=test)\n",
    "\n",
    "valid = open(\"../valid.txt\", 'w', encoding='utf-8')\n",
    "print(data_valid, file=valid)\n",
    "\n",
    "# triples_by_name = open(\"../triples_nyt10.txt\", 'w', encoding='utf-8')\n",
    "# print(all_triples_by_name, file=triples_by_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = entity2label_str.split('\\n')\n",
    "\n",
    "e2l_mapping = {}\n",
    "for line in lines:\n",
    "    e = line.split('\\t')[0]\n",
    "    l = line.split('\\t')[1]\n",
    "    e2l_mapping[e] = l\n",
    "\n",
    "\n",
    "## make tripes with words instead of ids\n",
    "import random\n",
    "# make train/test/valid data\n",
    "random.shuffle(data_corpus_new)\n",
    "\n",
    "data_train = \"\"\n",
    "data_test = \"\"\n",
    "data_valid = \"\"\n",
    "\n",
    "count = 0\n",
    "\n",
    "triple_set = set()\n",
    "triple_word_set = set()\n",
    "\n",
    "for item in data_corpus_new:\n",
    "    triple = item['head']['word'] + '\\t' + item['relation'] + '\\t' + item['tail']['word'] + '\\n'\n",
    "    if triple not in triple_set:\n",
    "        triple_set.add(triple)\n",
    "        count += 1\n",
    "        if count == 9:\n",
    "            data_test += triple\n",
    "            continue\n",
    "        if count == 10:\n",
    "            data_valid += triple\n",
    "            count = 0\n",
    "            continue\n",
    "        data_train += triple\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "train = open(\"./train_words.txt\", 'w', encoding='utf-8')\n",
    "print(data_train, file=train)\n",
    "\n",
    "test = open(\"./test_words.txt\", 'w', encoding='utf-8')\n",
    "print(data_test, file=test)\n",
    "\n",
    "valid = open(\"./valid_words.txt\", 'w', encoding='utf-8')\n",
    "print(data_valid, file=valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a9d979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, data_type=\"train\", reverse=False):\n",
    "    with open(\"%s%s.txt\" % (data_dir, data_type), \"r\") as f:\n",
    "        data = f.read().strip().split(\"\\n\")\n",
    "        data = [i.split('\\t') for i in data]\n",
    "        if reverse:\n",
    "            data += [[i[2], i[1]+\"_reverse\", i[0]] for i in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b432822a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld = load_data('../')\n",
    "['m.0d05w3', '/location/country/administrative_divisions', 'm.03h64'] in ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8568c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kgc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d0509d9aa81f2882b18eeb72d4d23c32cae9029e9b99f63cde94ba86c35ac78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
