{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdba84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! unset http_proxy\n",
    "# ! unset https_proxy\n",
    "\n",
    "# from openie import StanfordOpenIE\n",
    "# import os\n",
    "# os.environ[\"CORENLP_HOME\"] = '../stanford-corenlp/stanford-corenlp-full-2018-10-05'\n",
    "# import stanza\n",
    "# # Import client module\n",
    "# from stanza.server import CoreNLPClient\n",
    "\n",
    "# client = CoreNLPClient(timeout=150000000, be_quiet=True, annotators=['openie'], classpath='../stanford-corenlp/stanford-corenlp-full-2018-10-05/*')\n",
    "# client.start()\n",
    "# import time\n",
    "# time.sleep(10)\n",
    "# text = \"Colm J. Neilson , of Belle Harbor , Queens , said he thought the conductors\"\n",
    "# document = client.annotate(text, output_format='json')\n",
    "# triples = []\n",
    "# for sentence in document['sentences']:\n",
    "#     for triple in sentence['openie']:\n",
    "#         triples.append({\n",
    "#            'subject': triple['subject'],\n",
    "#            'relation': triple['relation'],\n",
    "#             'object': triple['object']\n",
    "#         })\n",
    "# print(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f7c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b8c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataset = wiki_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f126fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(wiki_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataset[0]['text'].split('.')[0] + '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d95d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ../../../data/pj20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1741985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "corpus_text = \"\"\n",
    "\n",
    "for context in tqdm(wiki_dataset):\n",
    "    split_text = context['text'].split('.')\n",
    "    for t in split_text:\n",
    "        line = t + '.\\n'\n",
    "        corpus_text = corpus_text + line\n",
    "    \n",
    "w_text = open(\"../../../data/pj20/corpus_text.txt\", 'w', encoding='utf-8')\n",
    "print(corpus_text, file=w_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The set of relation we're using\n",
    "valid_relation_set = set()\n",
    "\n",
    "rel_list = ['/people/person/nationality', '/location/location/contains', '/people/person/place_lived', \n",
    "            '/people/deceased_person/place_of_death', '/people/person/ethnicity', '/people/ethnicity/people',\n",
    "            '/business/person/company', '/people/person/religion', '/location/neighborhood/neighborhood_of',\n",
    "            '/business/company/founders', '/people/person/children', '/location/administrative_division/country',\n",
    "            '/location/country/administrative_divisions', '/business/company/place_founded', '/location/us_county/county_seat']\n",
    "\n",
    "for i in range(len(rel_list)):\n",
    "    valid_relation_set.add(rel_list[i])\n",
    "    \n",
    "valid_relation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d45509",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_triples_path = \"./prompt_mining/triples_nyt10.txt\"\n",
    "\n",
    "random_selected_triples = \"\"\n",
    "with open(original_triples_path) as f:\n",
    "    original_triples = f.readlines()\n",
    "\n",
    "count = 0\n",
    "for idx in range(len(original_triples)):\n",
    "    if '/people/deceased_person/place_of_death' in original_triples[idx]:\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            random_selected_triples = random_selected_triples + original_triples[idx]\n",
    "\n",
    "sp_path = \"./prompt_mining/triples_nyt10_people_death.txt\"\n",
    "sp_file = open(sp_path, 'w', encoding='utf-8')\n",
    "print(random_selected_triples, file=sp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a925bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Read the processes Wiki corpus =============== #\n",
    "corpus_path = \"../../../data/pj20/corpus_text_low.txt\"\n",
    "\n",
    "with open(corpus_path) as f:\n",
    "    corpus_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d7f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_path = \"./prompt_mining/triples_nyt10_people_death.txt\"\n",
    "\n",
    "mined_text = \"\"\n",
    "\n",
    "with open(triples_path) as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "for line in lines:\n",
    "    triple = line.split('\\t')\n",
    "    head, relation, tail = triple[0].replace('_', ' '), triple[1].split('/')[3].replace('_', ' '), triple[2][:-1].replace('_', ' ')\n",
    "    print('=======================')\n",
    "    print(head, relation, tail)\n",
    "    print('=======================')\n",
    "    for corpus_sentence in corpus_lines:\n",
    "        if (head in corpus_sentence) and (tail in corpus_sentence):\n",
    "#         and (relation in corpus_sentence) \\\n",
    "            mined_sentence = corpus_sentence.replace(head, '[X]').replace(tail, '[Y]')\n",
    "#             print(mined_sentence)\n",
    "            mined_text = mined_text + mined_sentence + '\\n'\n",
    "            \n",
    "mined_text_file = open(\"./prompt_mining/mined_people_death.txt\", 'w', encoding='utf-8')\n",
    "print(mined_text, file=mined_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ac964",
   "metadata": {},
   "outputs": [],
   "source": [
    "mined_text_file = open(\"./prompt_mining/mined_location_county.txt\", 'w', encoding='utf-8')\n",
    "print(mined_text, file=mined_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4304db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = wordnet.synsets('found')\n",
    "lemmas = set(chain.from_iterable([word.lemma_names() for word in synonyms]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bd2307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import webtext\n",
    "from nltk.probability import FreqDist\n",
    " \n",
    "! cp ./prompt_mining/mined_text_founders.txt /home/pj20/nltk_data/corpora/webtext/mined_text_founders.txt\n",
    "nltk.download('webtext')\n",
    "wt_words = webtext.words(\"mined_text_founders.txt\")\n",
    "data_analysis = nltk.FreqDist(wt_words)\n",
    " \n",
    "# Let's take the specific words only if their frequency is greater than 3.\n",
    "filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 3])\n",
    " \n",
    "# for key in sorted(filter_words):\n",
    "#     print(\"%s: %s\" % (key, filter_words[key]))\n",
    " \n",
    "data_analysis = nltk.FreqDist(filter_words)\n",
    "data_analysis.plot(40, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed5ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import webtext\n",
    "from nltk.probability import FreqDist\n",
    " \n",
    "! cp ./prompt_mining/mined_text_place_founded.txt /home/pj20/nltk_data/corpora/webtext/mined_text_place_founded.txt\n",
    "nltk.download('webtext')\n",
    "wt_words = webtext.words(\"mined_text_place_founded.txt\")\n",
    "data_analysis = nltk.FreqDist(wt_words)\n",
    " \n",
    "# Let's take the specific words only if their frequency is greater than 3.\n",
    "filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 3])\n",
    " \n",
    "data_analysis = nltk.FreqDist(filter_words)\n",
    "data_analysis.plot(40, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ad68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import webtext\n",
    "from nltk.probability import FreqDist\n",
    " \n",
    "! cp ./prompt_mining/mined_text_company.txt /home/pj20/nltk_data/corpora/webtext/mined_text_company.txt\n",
    "nltk.download('webtext')\n",
    "wt_words = webtext.words(\"mined_text_company.txt\")\n",
    "data_analysis = nltk.FreqDist(wt_words)\n",
    " \n",
    "# Let's take the specific words only if their frequency is greater than 3.\n",
    "filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 3])\n",
    " \n",
    "data_analysis = nltk.FreqDist(filter_words)\n",
    "data_analysis.plot(40, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a2dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import webtext\n",
    "from nltk.probability import FreqDist\n",
    " \n",
    "! cp ./prompt_mining/mined_administrative_division.txt /home/pj20/nltk_data/corpora/webtext/mined_administrative_division.txt\n",
    "nltk.download('webtext')\n",
    "wt_words = webtext.words(\"mined_administrative_division.txt\")\n",
    "data_analysis = nltk.FreqDist(wt_words)\n",
    " \n",
    "# Let's take the specific words only if their frequency is greater than 3.\n",
    "filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 3])\n",
    " \n",
    "data_analysis = nltk.FreqDist(filter_words)\n",
    "data_analysis.plot(40, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e70c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import webtext\n",
    "from nltk.probability import FreqDist\n",
    " \n",
    "! cp ./prompt_mining/mined_administrative_division_2.txt /home/pj20/nltk_data/corpora/webtext/mined_administrative_division_2.txt\n",
    "nltk.download('webtext')\n",
    "wt_words = webtext.words(\"mined_administrative_division_2.txt\")\n",
    "data_analysis = nltk.FreqDist(wt_words)\n",
    " \n",
    "# Let's take the specific words only if their frequency is greater than 3.\n",
    "filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 3])\n",
    " \n",
    "data_analysis = nltk.FreqDist(filter_words)\n",
    "data_analysis.plot(40, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5613e19f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
