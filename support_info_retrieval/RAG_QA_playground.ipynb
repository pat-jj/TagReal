{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f = open('../Volumes/Aux/Downloaded/Data-Upload/FB60K+NYT10/text/train.json')\n",
    "data_corpus = json.load(f)\n",
    "\n",
    "f = open('../Volumes/Aux/Downloaded/Data-Upload/FB60K+NYT10/text/test.json')\n",
    "tmp = json.load(f)\n",
    "\n",
    "data_corpus = data_corpus + tmp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'sen. charles e. schumer called on federal safety officials yesterday to reopen their investigation into the fatal crash of a passenger jet in belle_harbor , queens , because equipment failure , not pilot error , might have been the cause . ###END###\\n', 'head': {'word': 'queens', 'id': 'm.0ccvx'}, 'tail': {'word': 'belle_harbor', 'id': 'm.05gf08'}, 'relation': '/location/location/contains'}\n"
     ]
    }
   ],
   "source": [
    "corpus_filename = 'nyt10_corpus.json'\n",
    "chunked_filename = 'nyt10_corpus_chunked.json'\n",
    "\n",
    "with open(corpus_filename) as corpus_file:\n",
    "\n",
    "    with open(chunked_filename, \"w\") as chunked_file:\n",
    "        for jline in json.load(corpus_file):\n",
    "            print(jline)\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagModel\n",
    "import torch\n",
    "\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-base\")\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/rag-token-base\", index_name=\"exact\", use_dummy_dataset=True\n",
    ")\n",
    "# initialize with RagRetriever to do everything in one forward call\n",
    "model = RagModel.from_pretrained(\"facebook/rag-token-base\", retriever=retriever)\n",
    "\n",
    "inputs = tokenizer(\"How many people live in Paris?\", return_tensors=\"pt\")\n",
    "\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "# 1. Encode\n",
    "question_hidden_states = model.question_encoder(input_ids)[0]\n",
    "# 2. Retrieve\n",
    "docs_dict = retriever(input_ids.numpy(), question_hidden_states.detach().numpy(), return_tensors=\"pt\")\n",
    "doc_scores = torch.bmm(\n",
    "    question_hidden_states.unsqueeze(1), docs_dict[\"retrieved_doc_embeds\"].float().transpose(1, 2)\n",
    ").squeeze(1)\n",
    "\n",
    "# or directly generate\n",
    "generated = model.generate(\n",
    "    input_ids = inputs[\"input_ids\"],\n",
    "    context_input_ids=docs_dict[\"context_input_ids\"],\n",
    "    context_attention_mask=docs_dict[\"context_attention_mask\"],\n",
    "    doc_scores=doc_scores,\n",
    ")\n",
    "generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wiki_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ds_with_embeddings = wiki_dataset.map(lambda example: {'embeddings': ctx_encoder(**ctx_tokenizer(example[\"text\"],truncation=True, padding=\"longest\", return_tensors=\"pt\"))[0][0].numpy()})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
